# âœ¨ Aura Dial â€” Hand-Tracked Tone Shifter ğŸ¯

<div align="center">

[![Python](https://img.shields.io/badge/Python-3.8+-3776AB?style=for-the-badge&logo=python&logoColor=white)](https://www.python.org/)
[![OpenCV](https://img.shields.io/badge/OpenCV-4.5+-5C3EE8?style=for-the-badge&logo=opencv&logoColor=white)](https://opencv.org/)
[![MediaPipe](https://img.shields.io/badge/MediaPipe-0.10+-00D9FF?style=for-the-badge&logo=google&logoColor=white)](https://mediapipe.dev/)
[![License](https://img.shields.io/badge/License-MIT-green.svg?style=for-the-badge)](LICENSE)

</div>

## ğŸŒŸ Overview

Aura Dial is an **interactive hand-gesture-controlled demo** that transforms how you interact with LLM prompts! ğŸ¤– Using real-time webcam input and hand tracking, you can dynamically adjust the tone of AI responses by simply moving your finger across different "aura zones."

## âœ¨ Features

ğŸ–ï¸ **Hand Tracking** - Tracks your index fingertip using MediaPipe's advanced hand landmark detection

ğŸ¨ **Four Aura Blobs** - Visual representation of different tones:
- ğŸ“š **Academic** - Scholarly and formal
- ğŸ’¼ **Professional** - Business-appropriate
- ğŸ’¬ **Conversational** - Casual and friendly  
- ğŸ® **Playful** - Fun and creative

ğŸ“ **Dynamic Text Updates** - Real-time prompt panel showing tone-specific responses

ğŸ‘ï¸ **Visual Feedback** - Two white dots and connecting line track your finger position

ğŸ”Š **Text-to-Speech** - Optional voice feedback for tone changes (using pyttsx3)

## ğŸš€ Quick Start

### ğŸ“‹ Requirements

- ğŸ Python 3.8 or higher
- ğŸ“· Webcam access
- ğŸ’» macOS, Windows, or Linux

### ğŸ“¦ Installation

1ï¸âƒ£ **Clone the repository:**

```bash
git clone https://github.com/SOUMYA0023/Aura-Dial.git
cd Aura-Dial
```

2ï¸âƒ£ **Create and activate virtual environment:**

**For macOS/Linux:**
```bash
python3 -m venv .venv
source .venv/bin/activate
```

**For Windows (PowerShell):**
```powershell
python -m venv .venv
.\.venv\Scripts\Activate.ps1
```

3ï¸âƒ£ **Install dependencies:**

```bash
pip install -r requirements.txt
```

ğŸ’¡ **Troubleshooting:** If MediaPipe installation fails:

```bash
python -m pip install --upgrade pip setuptools wheel
pip install mediapipe
```

### â–¶ï¸ Run

```bash
python main.py
```

## ğŸ® How to Use

1. ğŸ‘‹ Hold your hand in front of the camera
2. â˜ï¸ Point your index finger toward different colored aura zones
3. ğŸ‘€ Watch the text panel update with tone-specific responses
4. ğŸ”„ Move between zones to blend different tones
5. âŒ¨ï¸ Press `q` or `Esc` to exit

## ğŸ› ï¸ Technology Stack

- **MediaPipe** - Hand landmark detection and tracking
- **OpenCV** - Real-time video processing and visualization
- **NumPy** - Numerical computations and array operations
- **pyttsx3** - Text-to-speech synthesis (optional)

## ğŸ“‚ Project Structure

```
Aura-Dial/
â”œâ”€â”€ ğŸ“„ main.py              # Main application code
â”œâ”€â”€ ğŸ“‹ requirements.txt     # Python dependencies
â”œâ”€â”€ ğŸ“– README.md           # Project documentation
â””â”€â”€ ğŸ“œ LICENSE             # MIT License
```

## ğŸ”® Future Enhancements

- ğŸ¤– Live LLM API integration (Gemini, OpenAI, etc.)
- ğŸ›ï¸ GUI for adjusting aura positions dynamically
- ğŸ™ï¸ Multiple TTS voice profiles
- ğŸŒˆ Custom tone creation
- ğŸ“Š Tone usage analytics
- ğŸ¨ Customizable visual themes

## ğŸ¤ Contributing

Contributions, issues, and feature requests are welcome! Feel free to check the [issues page](https://github.com/SOUMYA0023/Aura-Dial/issues).

## ğŸ“„ License

This project is licensed under the **MIT License** - see the [LICENSE](LICENSE) file for details.

## ğŸ‘¨â€ğŸ’» Author

**Soumya Suman Kar**
- ğŸ™ GitHub: [@SOUMYA0023](https://github.com/SOUMYA0023)
- ğŸ“§ Email: soumyasumankar23@gmail.com

## â­ Show Your Support

Give a â­ï¸ if this project helped you or you found it interesting!

---

<div align="center">

**Made with â¤ï¸ and Python**

</div>

